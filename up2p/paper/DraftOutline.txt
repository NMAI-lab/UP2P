  State of the art

Discuss existing work according to:

    * Deployment / Data Model:
          o Distributed Hash Tables: Several existing approaches use distributed hash tables (DHT) as the underlying data model for a distributed wiki. The Piki system uses a DHT to store article contents and search indexes, and sub-sets of the DHT key space are assigned to individual "owner" nodes in the system. This means that each article has a primary owner node to which all requests and edits for the particular article are routed. The primary owner also replicates the content to several backup nodes which can take over if the original owner node drops from the network. UniWiki builds on this approach by storing and propagating WOOT algorithm operations through a DHT which are used to construct consistent article content when a page is requested. The WOOT algorithm uses preconditions and partial ordering to ensure that concurrent modification operations are merged into a consistent state regardless of the order of arrival.
          o Unstructured: Each node hosts and edits articles on the local file system or in a local database and article sharing and propagation occurs above the data model layer. This propagation can occur through user initiated HTTP connections (DistriWiki), or through more complex methods. The Wooki and SWOOKI systems use an optimized WOOT algorithm combined with a probabilistic broadcast algorithm to replicate WOOT operations throughout the network. These operations are merged into article content at each node, and article content is stored as plain files (no DHT). Note that unlike the improved UniWiki DHT approach, this means that article content is totally replicated throughout the network and not just through a set of owner and replica nodes. This causes scalability issues with a large data set. 

    * Authority (Single vs Multiple): Existing work on distributed wikis aims to provide the user with a single authoritative version of an article. Either article edits and merging are sent to and managed by a single owner node (as allocated by the underlying DHT algorithm) or changes made by different users on different machines are propagated and merged throughout the network (using an algorithm such as WOOT) so that replicated copies at all nodes converge to the same version once the system is at rest. This approach maintains the traditional semantics of a wiki, in that each article only has one "most recent" authoritative version. In a multiple authority system each version of an article would represent a distinct resource that can be edited and shared independent of any other version of the article. 

    * Edits and Versioning: Existing approaches typically maintain a linear sequence of article versions handled by a single owner peer (Piki), or articles are stored as partially ordered sets of edit operations rather than explicit versions (Wooki, Swooki, UniWiki). When edit operations are stored the list of operations can be linearized to produce a coherent page to render to the user. However, in many cases the linearized version is system generated and has not been reviewed by a human user. An alternative approach used by existing work which does not provide a single authority (such as DistriWiki) is to assign a unique identifier to each version of a resource and leave conflict resolution up to the system users. 

    * Semantics of Wiki Links: As existing work is largely concerned with maintaining a single authoritative version of any given article the semantics of a wiki link mirror those of a link in a traditional wiki. Links fetch the "latest" version of an article (usually produced by the system automatically merging user edits on a single article). The implementation of a wiki link is more complicated than in a centralized approach as not all resources reside on the same server (and therefore may not be available to all nodes or may have delayed response times), but link semantics are largely unchanged. 

    * Trust :
          o Non-issue in most existing work (i.e. P2P wikis): Not relevant for methods that converge edits to a single version. Other related work that uses an unstructured model does not address trust issues.
          o P2P and trust : there is some existing work on the topic of trust in a more general "P2P networks" context. Relevant work includes PeerTrust, Tribler. The trust criteria used by PeerTrust are relevant to eCommerce, and less to our context: mostly PeerTrust uses explicit feedback upon transactions. Relevant criteria include the number of transactions, and their context : to us, frequent activity in the community may show that the user is an expert... thus an authority? The context of individual transactions could be useful, as users may be experts in certain domains. In Tribler, the two main criteria used are explicit manual annotation of trusted / distrusted users, and "taste buddies", i.e. we have an idea that users with similar tastes may be more trustworthy. Again, this may be relevant in the context of an encyclopedia. 

Our Proposal

Overview using the categories discussed in state of the art. Detail of each aspect in its own subsection:

    * Deployment / Data Model: UP2Pedia uses an unstructured deployment model. Each node stores articles in XML format in a local database, and articles are static once they have been saved. New versions of articles are saved as entirely separate files. A node's database is not shared or merged with any other node, and articles are propagated through manual search queries and downloads by system users (no content replication at the data layer).
    * Authority (Single vs Multiple): UP2Pedia is not concerned with producing a single authoritative version of an article. Instead, all versions produced by all users are kept in the system, and system users (with the help of the system's trust metrics) determine which article versions are most valuable (multiple authority approach).
    * Edits and Versioning: UP2Pedia handles edits by producing a new and distinct article version for every edit. This produces a tree of article versions rather than a linear sequence, as any version of an article can be edited at any time. Articles maintain a link to the parent article they originated from, as well as a list of links to articles further up the ancestry tree.
    * Semantics of Wiki Links: Since articles have a tree of versions rather than a linear sequence, the semantics of a wiki link must be different than in other approaches. A wiki link in UP2Pedia points to a specific article version (decided by the article author when the link is created). When a user follows a wiki link the system launches a search for the directly linked version as well as the complete sub-tree of edits that have been produced from the linked version. The user can then choose an appropriate version to view based on the resulting trust metrics.
    * Trust: Rather than converging article edits to a single version, UP2Pedia searches provide a number of trust metrics which users can consider in order to determine the most relevant article version to view. At this point theses metrics include:
          o Jaccard Index: A measure of similarity between the resources hosted by two nodes (intersection of the hosted resources / union). A remote node with many of the same resources should represent a user with similar views on article quality.
          o Network Distance: A remote node's distance in the Gnutella network graph. This metric relies on users manually adding trusted nodes to the local host cache.
          o Peer Popularity: The number of incoming connections to a remote peer. Peers which serve higher quality articles should garner more incoming connections as other users add the quality node to their host cache.
          o Resource Popularity: The total number of available sources for a resource. 

Implementation

UP2Pedia is implemented using the Universal Peer-to-Peer framework developed at Carleton University. U-P2P is implemented with Java ServerPages / Servlets and runs on the Apache Tomcat Servlet engine / web server. Apache Xindice is used as the local XML database.

    * Deployment / Data Model: Articles in UP2Pedia are stored in XML format in the local XML database. Article attachments are stored as plain files in the node's local file system. A "file mapper" database is used to map article attachments (as defined in the article XML) to valid file paths in the local file system.
    * Authority (Single vs Multiple): UP2Pedia does not attempt to merge article edits (and in fact the U-P2P framework itself does not provide any support for versioning). This means that every version of a UP2Pedia article is a distinct U-P2P resource which exists entirely independently of any other version. Users can download, share, and view any existing version of an article. Each version of an article also appears as a separate listing in search results with its own trust metrics independent of any other version.
    * Edits and Versioning: Article versioning information is stored in XML format in the contents of the articles themselves. These meta-data XML components are hidden from the user during editing and viewing operations. Each article stores a link to the URI of it's parent article, as well as all other articles in its chain of ancestors. UP2Pedia uses the graph query support of U-P2P to allow for complex queries based on article versioning (ex. "Find all sibling edits of this article" or "find all third generation child edits of this article").
    * Semantics of Wiki Links: A wiki link in UP2Pedia must fetch not only the linked resource, but the sub-tree of all edits of the linked resource. This is easily accomplished by performing a plain text search to see if the UP2P URI of the linked resource appears in the ancestry meta-data element of any other shared articles. Alternatively a graph based query can be used on either the ancestry or parent meta-data attributes (both methods are supported, but the plain text search is more straightforward / has better performance).
    * Trust: Trust metrics in UP2Pedia are either implemented by attaching additional meta-data to search response messages or by extracting information included in the standard Gnutella protocol messages.
          o Jaccard Index: When responding to a query the responding node includes a list of the resources it is hosting (hashed resource ids) which the querying node processes and uses to calculate a similarity rating.
          o Network Distance: This value is determined by the querying node when receiving a search response by checking the Gnutella protocol "hops" value on the incoming response. This allows users to effectively flag specific nodes as being trust worthy by adding them to their local host cache.
          o Peer Popularity: The number of incoming connections to a node is included as meta-data in search responses.
          o Resource Popularity: No additional meta-data is required for this metric, as a querying node simply tracks the number of query hits it receives for a specific resource. 

Evaluation and Validation

The evaluation and Validation should comprise two parts : a first study should evaluate the technical performance of the system (including reliability, scalability, etc.) w.r.t. an established baseline; a second study (perhaps coming first) should evaluate (as in "investigate, show") the behavior of the system w.r.t. its novel aspects, in order to show what consequences our unique approach generates.

    * Performance:
          o Network traffic: Uses Gnutella 0.4 protocol - No peer ever communicates with any peer other than its direct neighbours in the Gnutella network graph, except for file transfers which are handled with direct HTTP connections between the two transferring peers. Search queries are propagated with simply query flooding, and query and ping responses travel along the same network path they originated from.
          o Query response times: (also addresses scalability - will depend primarily on resource size and overall number of resources being shared in the active community) 

    * Usability: Talk about unique UI features (among distributed wikis) of U-P2P?
          o Dynamic generation of resource fields for search results
          o Dynamic generation of view, search and create pages for communities where these resources and not explicitly defined
          o Tree based browsers for database contents as well as article versions (UP2Pedia specific) 

    * Reliability:
          o Discuss effects of intermittent node connections on resource availability. Popular resources are spread and hosted on many nodes, and should remain available even with significant network churn.
          o Absolutely no central organization, so no centralized points of failure.
          o Limited host cache and lack of host sharing mechanism may be potential weakness. U-P2P nodes operate with a predefined XML host cache rather than web based caches traditionally used by Gnutella servents, and no persistent sharing of hosts occurs between nodes. A node discovery and sharing mechanism would likely be required if U-P2P were to be used with a large scale network. 

    * Scalability:
          o Number of peers: Operation response times should remain relatively unchanged by network size (other than small delay for additional routing hops). However, since U-P2P uses the Gnutella 0.4 protocol search queries use simple query flooding to propagate to all nodes. This means that the network traffic per node increases for each additional node in the network, causing scalability issues for a large network. The Gnutella 0.6 protocol uses query routing to improve scalability, but only supports plain text match searches on resource titles.
          o Number and Size of Shared Articles: Primarily affects database access response times. The XML database XPath queries are the primary local performance bottleneck, and the performance of these queries degrade as more data is added to the database (either by adding more total resources or increasing the average size of resources). Each community is stored as a separate database, and as such serving a large community will not affect the response time of queries on unrelated communities.
          o Number of Versions of a Given Article: Article meta-data will increase as revision count increases, producing the same effects as an increased average article size. In addition more effort is required from users to sort and consider article versions. Trust metrics will be very important as number of article revisions increases. 

    * Novelty of UP2Pedia 

The main difference between our approach and related work is that we do *not* attempt to merge different versions of a page, but rather have multiple versions of a page coexist, with the responsibility being on the user to choose which "version of the truth" he/she wants to adopt.

It is important to stress that our research goal is not to investigate the behavior of users faced with this type of versioning; whether they will make good use of it, ignore it and attempt to maintain a single authoritative version, etc. That, while very interesting, is relevant to social science, not to engineering / computer science, and should be kept out of the scope of this paper. What would be of interest to us would be, assuming we know the behavior of users, to investigate how that translates in terms of document propagation, distribution of versions (power laws, etc). Again, it is not fundamentally important in this study to know what the real behavior of the users will be, rather we can make a few assumptions, model different behaviors which our system makes possible and more or less likely (subjective), and investigate the consequences for the distribution / propagation of documents.

For this purpose we will (Alan in course SYSC5104) build a simulator to simulate (a) a P2P network, and (b) a user behaving as we assume she will behave, with a few tweakable and randomizable parameters perhaps. We then simulate many users interacting with the P2P network, and observe the effects. 